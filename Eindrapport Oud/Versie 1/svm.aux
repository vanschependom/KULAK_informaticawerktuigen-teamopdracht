\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\catcode `"\active 
\babel@aux{dutch}{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Support Vector Machines}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{tekst:svm}{{1}{1}{Support Vector Machines}{chapter.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Inleiding}{1}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Het scheidingsprincipe}{1}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Waarom SVM en geen andere classificatietechniek?}{1}{section.1.3}\protected@file@percent }
\citation{Luca_2022}
\citation{Luca_2022}
\citation{Ramo_2017}
\citation{Ramo_2017}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Deze figuur uit bron \cite  {Luca_2022} geeft een belangrijk verschil tussen neurale netwerken en SVM weer. Links wordt de werking van een neuraal netwerk geïllustreerd, rechts die van een SVM-model. Hierbij valt op het dat als je een nieuw punt zou classificeren, er bij neurale netwerken een veel grote kans was dat deze op het vorige model niet incorrect zou zijn, terwijl er bij SVM wel een veel grotere kans is dat het punt correct geclassificeerd zou worden. Deze figuur benadrukt dus ook het belang van de hyperplane bij SVM en zijn rol bij het voorkomen van \textit  {overfitting}. \relax }}{2}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:NeuralNetsVsSVM}{{1.1}{2}{Deze figuur uit bron \cite {Luca_2022} geeft een belangrijk verschil tussen neurale netwerken en SVM weer. Links wordt de werking van een neuraal netwerk geïllustreerd, rechts die van een SVM-model. Hierbij valt op het dat als je een nieuw punt zou classificeren, er bij neurale netwerken een veel grote kans was dat deze op het vorige model niet incorrect zou zijn, terwijl er bij SVM wel een veel grotere kans is dat het punt correct geclassificeerd zou worden. Deze figuur benadrukt dus ook het belang van de hyperplane bij SVM en zijn rol bij het voorkomen van \textit {overfitting}. \relax }{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1.2}{\ignorespaces Deze figuur uit bron \cite  {Ramo_2017} toont de vergelijking tussen SVM en logistische regressie. Hierbij valt het duidelijk op dat wanneer er sprake is van een \textit  {outlier}, logistische regressie de mist in gaat. Dit toont dan ook aan dat Support Vector Machines veel beter bestendig zijn tegen overfitting. \relax }}{2}{figure.caption.2}\protected@file@percent }
\newlabel{fig:LogistischeRegressieVsSVM}{{1.2}{2}{Deze figuur uit bron \cite {Ramo_2017} toont de vergelijking tussen SVM en logistische regressie. Hierbij valt het duidelijk op dat wanneer er sprake is van een \textit {outlier}, logistische regressie de mist in gaat. Dit toont dan ook aan dat Support Vector Machines veel beter bestendig zijn tegen overfitting. \relax }{figure.caption.2}{}}
\citation{bzdok2018machine}
\citation{bzdok2018machine}
\@writefile{lof}{\contentsline {figure}{\numberline {1.3}{\ignorespaces Deze figuur uit bron \cite  {bzdok2018machine} vergelijkt KNN-classificatie met SVM-classificatie. Bij de eerste van de twee technieken, ontstaan er twee 2 gebieden, die worden bepaald door naar de \(k\) dichtste buren te kijken van elk datapunt. Op de linkerfiguur is \(k=3\), op de rechterfiguur is \(k=7\). M.b.v. \textit  {cross-validation} werd bepaald dat \(k=7\) de optimale waarde is voor de metaparameter \(k\), met een accuraatheid van 87\%. De rechte die door de grafiek gaat, is de beslissingsgrens van SVM en de stippellijnen zijn de grenzen van de hyperplanes van SVM. Hierbij valt het duidelijk op dat SVM veel makkelijker te interpreteren is in vergelijking met KNN, terwijl de zekerheid van SVM nog steeds gelijk is aan 85\%. We boeten, door gebruik te maken van SVM, dus een heel klein beetje in op accuraatheid, maar dit is serieus in het voordeel van de interpreteerbaarheid van het model.\relax }}{3}{figure.caption.3}\protected@file@percent }
\newlabel{fig:KNNvsSVM}{{1.3}{3}{Deze figuur uit bron \cite {bzdok2018machine} vergelijkt KNN-classificatie met SVM-classificatie. Bij de eerste van de twee technieken, ontstaan er twee 2 gebieden, die worden bepaald door naar de \(k\) dichtste buren te kijken van elk datapunt. Op de linkerfiguur is \(k=3\), op de rechterfiguur is \(k=7\). M.b.v. \textit {cross-validation} werd bepaald dat \(k=7\) de optimale waarde is voor de metaparameter \(k\), met een accuraatheid van 87\%. De rechte die door de grafiek gaat, is de beslissingsgrens van SVM en de stippellijnen zijn de grenzen van de hyperplanes van SVM. Hierbij valt het duidelijk op dat SVM veel makkelijker te interpreteren is in vergelijking met KNN, terwijl de zekerheid van SVM nog steeds gelijk is aan 85\%. We boeten, door gebruik te maken van SVM, dus een heel klein beetje in op accuraatheid, maar dit is serieus in het voordeel van de interpreteerbaarheid van het model.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.4}Wiskundige berekening van het model}{3}{section.1.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.1}De hypervlakken}{3}{subsection.1.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.4}{\ignorespaces Twee lineair scheidbare wolken van punten. De kleuren van de punten duiden aan tot welke klasse ze behoren.\relax }}{4}{figure.caption.4}\protected@file@percent }
\newlabel{fig:svm}{{1.4}{4}{Twee lineair scheidbare wolken van punten. De kleuren van de punten duiden aan tot welke klasse ze behoren.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.2}Trainen van het model}{4}{subsection.1.4.2}\protected@file@percent }
\citation{enwiki:1183475870}
\citation{mediumarticle}
\citation{bzdok2018machine}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4.3}Regularisatieparameter}{5}{subsection.1.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1.5}{\ignorespaces De invloed van de metaparameter \(\lambda \) op het SVM-model. Een grote \(\lambda \) resulteert in een grote marge, een kleine \(\lambda \) resulteert in een kleine marge.\relax }}{6}{figure.caption.5}\protected@file@percent }
\newlabel{fig:lambda}{{1.5}{6}{De invloed van de metaparameter \(\lambda \) op het SVM-model. Een grote \(\lambda \) resulteert in een grote marge, een kleine \(\lambda \) resulteert in een kleine marge.\relax }{figure.caption.5}{}}
\gdef \@abspage@last{6}
